@article{vincent2022,
    author = {Vincent-Wayne Mitchell, William S. Harvey and Geoffrey Wood},
    title = {Where does all the ‘know how’ go? The role of tacit knowledge in research impact},
    journal = {Higher Education Research \& Development},
    volume = {41},
    number = {5},
    pages = {1664--1678},
    year = {2022},
    publisher = {Routledge},
    doi = {10.1080/07294360.2021.1937066},
    URL = {https://doi.org/10.1080/07294360.2021.1937066},
    eprint = {https://doi.org/10.1080/07294360.2021.1937066}
}

@phdthesis{wiharja2020iterative,
  title={An iterative approach for schema aware knowledge graph completion},
  author={Wiharja, Kemas Rahmat Saleh},
  year={2020},
  school={University of Aberdeen}
}

@article{Gao2021,
   abstract = {Under the background of economic globalisation, to promote the sustainable development of enterprises, sustainable innovation performance is explored for enterprises from the perspective of knowledge acquisition. Here, high-tech industry practitioners in Jiangsu Province are recruited for research of knowledge acquisition and continuous innovation using a QS (Questionnaire Survey). A total of 360 QSs are issued, and the QS items are developed based on a comprehensive collation of domestic and foreign literature and are mostly quoted from existing data. Consequently, a talent mining method is proposed for technological innovation based on the machine learning multi-layer perceptron model. The results show that there is a significant correlation between complementary knowledge and knowledge acquisition. Knowledge acquisition is significantly related to continuous innovation. Complementary knowledge is significantly related to continuous innovation. The high-tech industry has realised that knowledge will become the key to the success of the high-tech industry in the future.},
   author = {Bo Gao},
   doi = {10.1080/14778238.2021.1955631},
   issn = {14778246},
   journal = {Knowledge Management Research and Practice},
   title = {Exploration of talent mining based on machine learning and the influence of knowledge acquisition},
   year = {2021},
}

@article{Bem2022,
   abstract = {This paper provides a structured literature review (SLR) on knowledge management (KM), digital transformation (DT), and Industry 4.0, defining these new research streams’ interactions, ties, and interdependencies. Playing an essential role in a progressive discipline, our research summarises the state of the art of the past literature using rigorous methodological approach. The researchers adopt the Scopus database in their analysis and use the Bibliometrix R package. The analysis reveals 761 peer-reviewed English articles. The study shows several research clusters: KM and DT; KM and innovation ecosystems; KM and frontier technologies; and KM, decision-making, and Industry 4.0. Additionally, our paper contributes to identifying a flourishing field of research that has uncovered a previously unknown and exciting link between KM, DT, and the public sector. Finally, the article emphasises the crucial role of DT in KM development, addressing future research perspectives as quantitative and joint academics and practitioners’ analysis.},
   author = {Andreia de Bem Machado and Silvana Secinaro and Davide Calandra and Federico Lanzalonga},
   doi = {10.1080/14778238.2021.2015261},
   issn = {14778246},
   issue = {2},
   journal = {Knowledge Management Research and Practice},
   title = {Knowledge management and digital transformation for Industry 4.0: a structured literature review},
   volume = {20},
   year = {2022},
}

@book{Manning2008, 
    place={Cambridge}, 
    title={Introduction to Information Retrieval}, 
    publisher={Cambridge University Press}, 
    author={Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich}, 
    year={2008}
}

@inproceedings{Brown2020,
    author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    title = {Language models are few-shot learners},
    year = {2020},
    isbn = {9781713829546},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
    booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
    articleno = {159},
    numpages = {25},
    location = {Vancouver, BC, Canada},
    series = {NIPS '20}
}

@article{Lin2004,
   abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to auto- matically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of over- lapping units such as n-gram, word sequences, and word pairs between the computer-generated sum- mary to be evaluated and the ideal summaries cre- ated by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summariza- tion evaluation package and their evaluatio ns. Three of them have been used in the Document Under- standing\tConference\t(DUC)\t2004,\ta\tlarge -scale summarization evaluation sponsored by NIST.},
   author = {C Y Lin},
   issue = {1},
   journal = {Proceedings of the workshop on text summarization branches out (WAS 2004)},
   title = {Rouge: A package for automatic evaluation of summaries},
   year = {2004},
}

@article{Hogan2021,
   abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
   author = {Aidan Hogan and Eva Blomqvist and Michael Cochez and Claudia D'Amato and Gerard De Melo and Claudio Gutierrez and Sabrina Kirrane and José Emilio Labra Gayo and Roberto Navigli and Sebastian Neumaier and Axel Cyrille Ngonga Ngomo and Axel Polleres and Sabbir M. Rashid and Anisa Rula and Lukas Schmelzeisen and Juan Sequeda and Steffen Staab and Antoine Zimmermann},
   doi = {10.1145/3447772},
   issn = {15577341},
   issue = {4},
   journal = {ACM Computing Surveys},
   title = {Knowledge graphs},
   volume = {54},
   year = {2021},
}

@article{Paulheim2017,
   abstract = {In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term Knowledge Graph in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
   author = {Heiko Paulheim},
   doi = {10.3233/SW-160218},
   issn = {22104968},
   issue = {3},
   journal = {Semantic Web},
   title = {Knowledge graph refinement: A survey of approaches and evaluation methods},
   volume = {8},
   year = {2017},
}

@article{Noy2019,
   abstract = {KNOWLEDGE GRAPHS ARE critical to many enterprises today: They provide the structured data and factual knowledge that drive many products and make them more intelligent and “magical.” In general, a knowledge graph describes objects of interest and connections between them. For example, a knowledge graph may have nodes for a movie, the actors in this movie, the director, and so on. Each node may have properties such as an actor’s name and age. There may be nodes for multiple movies involving a particular actor. The user can then traverse the knowledge graph to collect information on all the movies in which the actor appeared or, if applicable, directed.},
   author = {Natasha Noy and Yuqing Gao and Anshu Jain and Anant Narayanan and Alan Patterson and Jamie Taylor},
   doi = {10.1145/3331166},
   issn = {15577317},
   issue = {8},
   journal = {Communications of the ACM},
   title = {Industry-scale knowledge graphs: Lessons and challenges},
   volume = {62},
   year = {2019},
}

@inproceedings{Zhang2020,
   abstract = {We propose BERTSCORE, an automatic evaluation metric for text generation. Analogously to common metrics, BERTSCORE computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTSCORE correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTSCORE is more robust to challenging examples when compared to existing metrics.},
   author = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
   booktitle = {8th International Conference on Learning Representations, ICLR 2020},
   title = {BERTSCORE: EVALUATING TEXT GENERATION WITH BERT},
   year = {2020},
}

@inproceedings{Goyal2020,
   abstract = {Despite significant progress in text generation models, a serious limitation is their tendency to produce text that is factually inconsistent with information in the input. Recent work has studied whether textual entailment systems can be used to identify factual errors; however, these sentence-level entailment models are trained to solve a different problem than generation filtering and they do not localize which part of a generation is non-factual. In this paper, we propose a new formulation of entailment that decomposes it at the level of dependency arcs. Rather than focusing on aggregate decisions, we instead ask whether the semantic relationship manifested by individual dependency arcs in the generated output is supported by the input. Human judgments on this task are difficult to obtain; we therefore propose a method to automatically create data based on existing entailment or paraphrase corpora. Experiments show that our dependency arc entailment model trained on this data can identify factual inconsistencies in paraphrasing and summarization better than sentence-level methods or those based on question generation, while additionally localizing the erroneous parts of the generation.},
   author = {Tanya Goyal and Greg Durrett},
   doi = {10.18653/v1/2020.findings-emnlp.322},
   booktitle = {Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020},
   title = {Evaluating factuality in generation with dependency-level entailment},
   year = {2020},
}

@unknown{Guo2024,
    author = {Guo, Tong},
    year = {2024},
    month = {02},
    pages = {},
    title = {Improving Text Generation for Product Description via Human Behaviour},
    doi = {10.36227/techrxiv.170846525.55626336/v1}
}

@article{Celikyilmaz2020,
  title={Evaluation of Text Generation: A Survey},
  author={Asli Celikyilmaz and Elizabeth Clark and Jianfeng Gao},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.14799},
  url={https://api.semanticscholar.org/CorpusID:220128348}
}

@misc{deepLearning2023LangChain,
  author = {{DeepLearning.AI}},
  title = {LangChain: Chat with Your Data},
  year = {2023},
  url = {https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/},
  note = {Accessed: 2024-09-16}
}

@article{kiran2019,
    author = {Kiran Adnan and Rehan Akbar},
    title ={Limitations of information extraction methods and techniques for heterogeneous unstructured big data},
    journal = {International Journal of Engineering Business Management},
    volume = {11},
    number = {},
    pages = {1847979019890771},
    year = {2019},
    doi = {10.1177/1847979019890771},
    URL = {https://doi.org/10.1177/1847979019890771}
}

@unknown{Krause2024,
    author = {Krause, Stefanie and Panchal, Bhumi and Ubhe, Nikhil},
    year = {2024},
    month = {04},
    pages = {},
    title = {The Evolution of Learning: Assessing the Transformative Impact of Generative AI on Higher Education},
    doi = {10.48550/ARXIV.2404.10551}
}






